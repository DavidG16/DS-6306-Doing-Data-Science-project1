---
title: "project 1"
author: "David Grijalva"
date: "10/5/2020"
output:
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Load Libraries
```{r}
library(tm) 
library(tidyr)
library(plyr)
library(dplyr)
library(tidyverse)
library(caret)
library(class)
library(e1071)
library(data.table)
library(gganimate)
library(GGally)
require(ggthemes)


```

# Load the CSVs
```{r}
breweries = read.csv("./Breweries.csv")
beers = read.csv("./Beers.csv")

```


# Question 1
```{r}
breweries_per_state = breweries %>% count(State)
breweries_per_state %>% ggplot() + geom_bar(aes(State, n), fill="steelblue" ,stat = 'identity')+ geom_text(stat = "count", aes(State, label = n, vjust=-0.1),size = 2.5, color = "darkred")+ labs(x='', y="Number of Breweries", title='Number of Breweries per State') +theme_economist()+ theme(axis.text.x = element_text(angle = 60, hjust = 1,size = 6))

```

```{r}
# Building Heatmap of US for number of breweries in each state
breweries_per_state_1 = breweries_per_state
breweries_per_state_1$State = trimws(breweries_per_state_1$State,which=c("both"),whitespace = "[ ]")

lookup = data.frame(abbr = state.abb, State = state.name)

breweries_per_state_1 = merge(breweries_per_state_1, lookup, by.x="State", by.y="abbr")
breweries_per_state_1$StateLower = tolower(breweries_per_state_1$State.y)

states = map_data("state")

map.df = merge(states,breweries_per_state_1, by.x = "region", by.y ="StateLower",  all.x=T)
map.df = map.df[order(map.df$order),]
colnames(map.df)[8] = "Num.of.Breweries"

ggplot(map.df, aes(x=long,y=lat,group=group))+
  geom_polygon(aes(fill=Num.of.Breweries))+
  geom_path()+ 
  scale_fill_gradientn(colours=rev(heat.colors(5)), na.value="grey90") + ggtitle("Breweries by State")+theme_map()
```
This heatmap helps to easily identify which states are the ones with the most breweries. 
The two states with the most breweries are California and Colorado, with 39 and 47 breweries respectively. 


# Question 2 - Merge beer data with the breweries data 
```{r}
data = merge(x=breweries, y=beers, by.y ="Brewery_id" , by.x = "Brew_ID")
data = data%>%rename(Beer.Name=Name.y, Brewery = Name.x)
nrow(data)
head(data,6)
tail(data,6)
```
The breweries and beers dataframe are merged in the Brewery_id and Brew_ID columns respectively. This merge results in a dataframe of 2410 rows. 



# Questions 3 - Handle missing values

First lets look at how many missing values we have per column. If the columns are continous values, let's also look at
the distribution so we can figure out how to best handle them. 
```{r}
missing.values <- data %>%
    gather(key = "key", value = "val") %>%
    mutate(is.missing = is.na(val)) %>%
    group_by(key, is.missing) %>%
    summarise(num.missing = n()) %>%
    filter(is.missing==T) %>%
    select(-is.missing) %>%
    arrange(desc(num.missing))


missing.values %>% ggplot() + geom_bar(aes(x=key, y=num.missing), fill="steelblue",stat = 'identity') + geom_text(stat = "count", aes(key, label = num.missing, vjust=-0.2),size = 4, color = "white")+
   labs(x='', y="Number of missing values", title='Number of missing values') +theme_economist()+
   theme(axis.text.x = element_text(angle = 0, hjust = 1))

data %>% ggplot() + geom_histogram(aes(x=ABV), fill="steelblue") +theme_economist()+
   labs(x="ABV", y="Count", title="ABV Distribution")
data %>% ggplot() + geom_histogram(aes(x=IBU), fill="steelblue") +theme_economist()+
   labs(x="IBU", y="Count", title="IBU Distribution")
```



From the graphs above we notice that the only columns with missing values are ABV and IBU. 
From having a quick visual inspection into the distribution of each, we notice that both these are somewhat right skewed, becuase of this it will be best if we use medians instead of means to impute the data. 
```{r}
# Get rid of special characters in the beer styles
data$Style = gsub("[^0-9A-Za-z' ]"," " , data$Style ,ignore.case = TRUE)

#Deal with NA in IBU
# Finds the median value per beer style
meanIBU = matrix(nrow = 100)
styles = list()
for (i in 1:length(unique(data$Style)) )
{
  beer_style = unique(data$Style)[i]
  ibu_mean = mean(data[grep(beer_style, data$Style, ignore.case = T),]$IBU,na.rm = T )
  meanIBU[i] = ibu_mean
  styles[[i]] = beer_style
}

# Create a new styles dataframe with the IBU medians per beer style
styles_impute = data.frame(IBU=meanIBU, Style = matrix(unlist(styles), nrow=length(styles), byrow=T))

# merge the beer styles median IBU dataframe with the working dataframe on style name 
impute_data = merge(data, styles_impute, by.x="Style", by.y="Style") 

# If NA in original IBU value, then use median IBU per style, else use original value 
impute_data = impute_data %>% mutate(imputed_IBU = ifelse(is.na(IBU.x) == TRUE,IBU.y,IBU.x))
# Impute any impute_data value with the median for the ABV and imputed_IBU columns
impute_data= impute_data %>% mutate_at(vars(ABV,imputed_IBU),~ifelse(is.na(.x), median(.x, na.rm = TRUE), .x))

# Get rid of the 5 rows without a beer style. 
impute_data = impute_data%>% filter(!Style=="")

# Drop redundant columns
drops =  c("IBU.x","IBU.y")
impute_data = impute_data[ , !(names(impute_data) %in% drops)]


```
Several steps went into the handling the missing data for ABV and IBU. 
Each type of beer style has its unique bitterness which might vary a bit from brand to brand but not much and seeing that there is 100 beer styles and more than 1,000 missing values we felt that the best approach was not impute the missing IBU with the median IBU from all the known data. Instead we calculated the median IBU per beer style and impute each missing IBU value with the median IBU value for that style. 
Some beers styles IBU's had all missing, for this edge case we impute the IBU with the median value from all styles. 
We believe this way of imputing IBU's is more representative to what the real distribution of IBU's would be.
Since there were only 62 missing values for ABV we decided to impute this with the median ABV value from all beers. 

There were also 5 beers with a blank style. We decided to drop these. 



# Question 4 - Compute the median alcohol content and international bitterness unit for each state. Plot a bar chart to compare.
```{r}

mean_abv_state = aggregate(impute_data[, 8], list(impute_data$State), median)
mean_ibu_state = aggregate(impute_data[, 10], list(impute_data$State), median)
mean_ibu_state %>% ggplot() + geom_bar(aes(Group.1, x),fill="steelblue", stat = 'identity') +
    labs(x='', y="IBU Value", title='IBU by State') + theme_economist()+
  theme(axis.text.x = element_text(angle = 60, hjust = 1, size=6))
mean_abv_state %>% ggplot() + geom_bar(aes(Group.1, x), fill="steelblue", stat = 'identity') +
    labs(x='', y="ABV Value", title='ABV by State') + theme_economist()+ 
  theme(axis.text.x = element_text(angle = 60, hjust = 1, size = 6 ))

```



The median ABV by state seems to have less variance than the medial IBU by state. 


# Question 5 - Which state has the maximum alcoholic (ABV) beer? Which state has the most bitter (IBU) beer?
```{r}
impute_data %>% filter(ABV==max(ABV))
impute_data %>% filter(imputed_IBU==max(imputed_IBU))

```


The state with the maximum ABV in a beer is CO.  The beer is the Lee Hill Series Vol. 5 - Belgian Style Quadrupel Ale with an ABV of 0.128. 
The state with the maximum  IBU in a beer is OR. The beer is the Bitter Bitch Imperial IPA with an IBU of 138. 


# Question 6 - Comment on the summary statistics and distribution of the ABV variable.
```{r}
summary(impute_data$ABV)
impute_data %>% ggplot() + geom_histogram(aes(ABV), fill="steelblue") +
  labs(x='ABV Value', y="count", title='ABV Distribution') + theme_economist()
impute_data %>% ggplot() + geom_boxplot(aes(ABV), fill="steelblue") + theme_economist()+
  labs(x='ABV Value', title='ABV Boxplot')
```

It appears that the distribution of the ABV variable is slightly right skewed.  
The 5 number summary goes as follow:   
Min: 0.001  
1st. Qu: 0.05  
Median: 0.05600  
Mean: 0.05968  
3rd Qu: 0.06700  
Max: 0.12800  


# Question 7 - Is there an apparent relationship between the bitterness of the beer and its alcoholic content? Draw a scatter plot.  Make your best judgment of a relationship and EXPLAIN your answer.
```{r}
impute_data %>% ggplot() + geom_point(aes(x=ABV, y=imputed_IBU), color="steelblue", size = 1.5) + geom_smooth(aes(x=ABV, y=imputed_IBU),method = "lm")+ labs(x='ABV Value', y="IBU Value", title="Relation between IBU and ABV") + theme_economist() + annotate("text",x=0.12, y=135, label="R-Square: 33.67%", color="darkred", size=4)

cor(impute_data$ABV, impute_data$imputed_IBU, method = "pearson")

lm1<-lm(ABV~imputed_IBU, data = impute_data)
summary(lm1)
```


From looking at the scatterplot above it seems that there is a positive relation between ABV and IBU variables. The higher the ABV the most likely the IBU will be.  Calculating the Spearman correlation we get a coeficient of 0.56 that indicates a medium-high correlation between the variables. This suggest some evidence that the more alcohol content in the beer the bitter it will be. 


# Question 8 - Budweiser would also like to investigate the difference with respect to IBU and ABV between IPAs (India Pale Ales) and other types of Ale (any beer with “Ale” in its name other than IPA).  You decide to use KNN classification to investigate this relationship.  Provide statistical evidence one way or the other. You can of course assume your audience is comfortable with percentages … KNN is very easy to understand conceptually.
```{r}
# Crete new ipa_ale column based on regex from the beer style column

impute_data$ipa_ale = ifelse(grepl("ipa", impute_data$Style, ignore.case = T), "ipa", 
         ifelse(grepl("ale", impute_data$Style, ignore.case = T), "ale", "Other"))
ale_ipa = impute_data %>% filter(!ipa_ale=="Other")


```

To asses the relation between IBU and ABV between IPA and Ales we will first need to create a column with classifies the beers between "ale", "ipa", and "other". We will then filter out the "other" variable to from the dataset. This will result in a dataset containing only "ale" and "ipa" labels.  

```{r}

#Choose the best K
set.seed(12)
splitPerc = .70
trainIndices = sample(1:dim(ale_ipa)[1],round(splitPerc * dim(ale_ipa)[1]))
train = ale_ipa[trainIndices,]
test = ale_ipa[-trainIndices,]


iterations = 50
accs = data.frame(accuracy = numeric(iterations), k = numeric(iterations))

for(i in 1:iterations)
{
  
  classification = knn(train[,c(8,10)], test[,c(8,10)],train$ipa_ale,k=i)
  cm = confusionMatrix(table(test$ipa_ale, classification ), positive="ale")
  

  accs$accuracy[i] = cm$overall[1]
  accs$k[i] = i
}

plot(accs$k,accs$accuracy, type = "l", xlab = "k")
axis(side=1, at=c(0:50, 5))
box()


```

Run 50 iterations of the knn classier to choose the K with the highest accuracy classifying betweemn "ale" and "ipa". It appears that the best K its 5. We will just this K to run different train/test splits. 


```{r}
# Run 1000 iterations  on different train/test sets. We will compute the average accuracy, specificity and Sensitivity.
iterations = 1000
masterAcc = matrix(nrow = iterations)
masterSensitivity = matrix(nrow = iterations)
masterSpecificity = matrix(nrow = iterations)
splitPerc = .7 #Training / Test split Percentage
for(j in 1:iterations)
{
  splitPerc = .70
  set.seed(j*49+15)
  trainIndices = sample(1:dim(ale_ipa)[1],round(splitPerc * dim(ale_ipa)[1]))
  train = ale_ipa[trainIndices,]
  test = ale_ipa[-trainIndices,]
  
  classification = knn(train[,c(8,10)], test[,c(8,10)],train$ipa_ale,k=5)
  cm = confusionMatrix(table(test$ipa_ale, classification ), positive="ale")
  
  
  masterAcc[j] = cm$overall[1]
  masterSpecificity[j] = cm$byClass[2]
  masterSensitivity[j] = cm$byClass[1]
  
}


MeanAcc = colMeans(masterAcc)
MeanSpecificity = colMeans(masterSpecificity)
MeanSensitivity = colMeans(masterSensitivity)

MeanAcc
MeanSpecificity
MeanSensitivity

```
```{r}

splitPerc = .70
set.seed(j*49+15)
trainIndices = sample(1:dim(ale_ipa)[1],round(splitPerc * dim(ale_ipa)[1]))
train = ale_ipa[trainIndices,]
test = ale_ipa[-trainIndices,]
  
  

# Do knn
fit = knn(train[,c(8,10)], test[,c(8,10)],train$ipa_ale,k=5)

# Create a dataframe to simplify charting
plot.df = data.frame(test, predicted = fit)



# First use Convex hull to determine boundary points of each cluster
plot.df1 = data.frame(x = plot.df$imputed_IBU, 
                      y = plot.df$ABV, 
                      predicted = plot.df$predicted)

find_hull = function(df) df[chull(df$x, df$y), ]
boundary = ddply(plot.df1, .variables = "predicted", .fun = find_hull)

ggplot(plot.df, aes(imputed_IBU, ABV, color = predicted, fill = predicted)) + 
  geom_point(size = 2) + 
  geom_polygon(data = boundary, aes(x,y), alpha = 0.5) + ggtitle("KNN Clusters Boundery") + theme_economist()

# plot source: https://stackoverflow.com/questions/35402850/how-to-plot-knn-clusters-boundaries-in-r
```

We run 1,000 iterations of different train/test splits with k=5 to get the average accuracy, sensitivity and specificity. 

Mean Accuracy: 0.9048203  
Mean Specificity:0.8699283  
Mean Sensitivity: 0.9247805  

Base on these results, it appears that the variables ABV and IBU are highly related with the Ale and IPA style beers. 


# Question 9 - Knock their socks off!  Find one other useful inference from the data that you feel Budweiser may be able to find value in.  You must convince them why it is important and back up your conviction with appropriate statistical evidence. 
```{r}
#
library(wordcloud)
#install.packages("RColorBrewer")
library(RColorBrewer)
#install.packages("wordcloud2")
library(wordcloud2)
#install.packages("tm")
library(tm)
text = as.vector(impute_data['Style'])
docs = Corpus(VectorSource(text))
docs = docs %>%
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>%
  tm_map(stripWhitespace)
docs = tm_map(docs, content_transformer(tolower))
docs = tm_map(docs, removeWords, stopwords("english"))
tdm = TermDocumentMatrix(docs) 
matrix = as.matrix(tdm) 
words = sort(rowSums(matrix),decreasing=TRUE) 
df_style = data.frame(word = names(words),freq=words)


text = as.vector(impute_data$Beer.Name)
docs = Corpus(VectorSource(text))
docs = docs %>%
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>%
  tm_map(stripWhitespace)
docs = tm_map(docs, content_transformer(tolower))
docs = tm_map(docs, removeWords, stopwords("english"))
tdm = TermDocumentMatrix(docs) 
matrix = as.matrix(tdm) 
words = sort(rowSums(matrix),decreasing=TRUE) 
df_name = data.frame(word = names(words),freq=words)

```

```{r}
wordcloud(words = df_style$word, freq = df_style$freq, min.freq = 1,max.words=100, random.order=FALSE, rot.per=0.35,colors=brewer.pal(8, "Dark2"))


set.seed(1234) # for reproducibility 
wordcloud(words = df_name$word, freq = df_name$freq, min.freq = 1,max.words=100, random.order=FALSE, rot.per=0.35,colors=brewer.pal(6, "Dark2"))

```